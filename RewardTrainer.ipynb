{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/AlignmentTuning/blob/main/RewardTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91e0c421-117d-4f74-a3d6-c335f860d90c"
      },
      "source": [
        "# Reward Modeling using `trl` library\n",
        "\n",
        "## What is Reward model\n",
        "In the context of **large language models (LLMs)**, a **reward model** is used to **rate different responses** the model can give, based on how helpful, safe, or relevant they are.\n",
        "\n",
        "**Purpose:** It helps train the LLM to give **better answers** by rewarding good responses and discouraging bad ones—like teaching the model what humans prefer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccb0cd5-cec3-4f40-ae5b-34ff6a752b6d"
      },
      "source": [
        "## __Table of Contents__\n",
        "\n",
        "<ol>\n",
        "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
        "    </ul>\n",
        "    <li><a href=\"#Setup\">Setup</a></li>\n",
        "    <ul>\n",
        "        <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
        "        <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
        "        <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
        "    </ul>\n",
        "    <li><a href=\"#Dataset\">Data set</a></li>\n",
        "    <ul>\n",
        "        <li><a href=\"#Dataset-features\">Data set features</a></li>\n",
        "    </ul>\n",
        "    <li><a href=\"#Model-and-tokenizer-Setup\">Model and tokenizer setup</a></li>\n",
        "    <li><a href=\"#Training Reward Model\">Training Reward Model</a></li>\n",
        "    <ul>\n",
        "      <li><a href=\"#PreProcessing the Data\">PreProcessing the Data</a></li>\n",
        "        <li><a href=\"#LoRA-configuration\">LoRA configuration</a></li>\n",
        "        <li><a href=\"#Training-arguments\">Training arguments</a></li>\n",
        "        <li><a href=\"#Reward-Trainer\">Reward trainer</a></li>\n",
        "    </ul>\n",
        "    <li><a href=\"#Evaluating-the-model\">Evaluating the model</a></li>\n",
        "    \n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8b62068-dc3e-4f1c-81bf-54f29fce992a"
      },
      "source": [
        "#1. Objectives\n",
        "By the end of this tutorial, you will be able to:\n",
        "- Grasp the fundamentals of how reward modeling guides language model behavior in machine learning.\n",
        "\n",
        "- Work with real-world datasets by cleaning and preparing them for use in reward-based training tasks.\n",
        "\n",
        "- Configure a GPT-2 model specifically for sequence classification workflows.\n",
        "\n",
        "- Process and tokenize text data to make it compatible with transformer-based models.\n",
        "\n",
        "- Assess model performance by comparing and ranking pairs of generated responses.\n",
        "\n",
        "- Apply both preprocessing and evaluation methods to various data segments.\n",
        "\n",
        "- Deepen your understanding of key principles behind transformers and how they relate to reward learning.\n",
        "\n",
        "- Use special tokens in the tokenizer and adjust model settings to accommodate them effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95969f95-43c4-402b-9530-6f62fae120b3"
      },
      "source": [
        "#2. Setup\n",
        "## Installing required libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20822d83-7632-4779-8f71-fd4277e22e61"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0  transformers==4.31.0 trl==0.5.0\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83b38605-bddf-4f0e-8dc6-1fddb4d602e9"
      },
      "source": [
        "## Importing required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ac79dc-dc92-4494-bb44-2eb6724dacd4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, TrainingArguments\n",
        "from peft import LoraConfig, TaskType\n",
        "from transformers import TrainingArguments\n",
        "from trl import RewardTrainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1eb7412-4524-47ec-a01b-8c408ff70d74"
      },
      "source": [
        "## Defining helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05002cc3-059f-45da-bb85-1701f7ffa782"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, file_path):\n",
        "    \"\"\"\n",
        "    Save a dictionary to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The dictionary to save.\n",
        "        file_path (str): The path to the JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(f\"Data successfully saved to {file_path}\")\n",
        "\n",
        "\n",
        "def load_from_json(file_path):\n",
        "    \"\"\"\n",
        "    Load data from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: The data loaded from the JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfa7244-2f6a-4ffc-a8d0-0609502e668a"
      },
      "source": [
        "\n",
        "#3. Data set\n",
        "\n",
        "For this turorial we will use the Dahoas/synthetic-instruct-gptj-pairwise data set. In this dataset each data point consist of prompt with a pair of Good and Bad response. The main purpose is to train models to distinguish between better and worse responses.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "085cd009-e413-4e13-bbe2-b94450774f53"
      },
      "outputs": [],
      "source": [
        "# Load the Dahoas/synthetic-instruct-gptj-pairwise dataset\n",
        "dataset = load_dataset(\"Dahoas/synthetic-instruct-gptj-pairwise\")\n",
        "# Display the dataset\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76fafc3a-fb55-4a87-806f-c42ad3e63385"
      },
      "source": [
        "### Data set features\n",
        "\n",
        "To get a better understanding of the data set, let's inspect a few samples. each data point is structured as.\n",
        "\n",
        "`Prompt:` The input text or question provided to the model to generate a response.\n",
        "\n",
        "`Chosen:` The response that is considered better or more appropriate for the given prompt.\n",
        "\n",
        "`Rejected:` The response that is deemed less suitable or lower in quality compared to the chosen one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "060f70c5-13e0-43a8-a2dc-dccda8870582"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print('prompt')\n",
        "    print(dataset[\"train\"][i]['prompt'],'\\n')\n",
        "\n",
        "    print('chosen')\n",
        "    print(dataset[ 'train'][i]['chosen'],'\\n')\n",
        "\n",
        "    print('rejected')\n",
        "    print(dataset[ 'train'][i]['rejected'],'\\n')\n",
        "    print('**********************************************\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ecc6a6-5fa3-40f5-808a-6d391b5f60b1"
      },
      "source": [
        "#4. Model and tokenizer setup\n",
        "In this section, you set up the tokenizer and the model for training. You can use the GPT-2 model for sequence classification, which helps in determining the quality of responses.\n",
        "\n",
        "Next, specify the model name or path as \"gpt2\". To initialize the tokenizer and model, use `GPT2Tokenizer.from_pretrained` and `GPT2ForSequenceClassification.from_pretrained`, respectively, with `num_labels` set to 1 for ranking (a numerical score value). To handle padding, set the `pad_token` of the tokenizer to be the same as the `eos_token` (end-of-sequence token). Similarly, configure the model to use the `eos_token_id` as the `pad_token_id`. This setup ensures that the tokenizer and model are correctly initialized and prepared for sequence classification tasks with GPT-2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc849b96-dd7e-4b07-a330-f9394b900816"
      },
      "outputs": [],
      "source": [
        "# Define the model name or path\n",
        "model_name_or_path = \"gpt2\"\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "model = GPT2ForSequenceClassification.from_pretrained(model_name_or_path, num_labels=1)\n",
        "\n",
        "# Add special tokens if necessary\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Define the maximum length\n",
        "max_length = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8144de96-4e0a-4b0e-9e5f-657cf1585164"
      },
      "source": [
        "#5. Training Reward Model\n",
        "## Preprocessing the data\n",
        "Next, preprocess the data set for training. Then combine the prompt with the chosen and rejected responses into a format suitable for input into the model. This process helps create clear input-output pairs for the model to learn from.\n",
        "\n",
        "`Lambda Function`: Define a lambda function `get_format` that takes the data set and a response type (chosen or rejected) and combines the prompt with the respective response. Each entry is formatted as a dialogue between \"Human\" and \"Assistant\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8a7ef0b-5fa1-4930-99ee-63f950acfe38"
      },
      "outputs": [],
      "source": [
        "def get_format(dataset, res):\n",
        "    result = []\n",
        "    for prompt, resp in zip(dataset[\"train\"][\"prompt\"], dataset[\"train\"][res]):\n",
        "        formatted = f\"\\n\\nHuman: {prompt}\\n\\nAssistant: {resp}\"\n",
        "        result.append(formatted)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d9a1db7-9fc2-43d4-874d-e880d3d82efa"
      },
      "source": [
        "`Chosen Samples`: Apply the `get_res` function to create a list of chosen samples.\n",
        "\n",
        "`Rejected Samples`: Similarly, create a list of rejected samples using the same function.\n",
        "\n",
        "After applying the function,  you get the following results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd6b2184-40df-4e5b-a45b-dee74f6e8cd4"
      },
      "outputs": [],
      "source": [
        "chosen_samples=get_format( dataset,'chosen')\n",
        "rejected_samples=get_format( dataset,'rejected')\n",
        "print('chosen',chosen_samples[0])\n",
        "print('rejected',rejected_samples[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9637498-9e9f-42e1-81f1-0a8ca5d4eadf"
      },
      "source": [
        "To facilitate the training process, create new columns in the data set that combine the prompt with chosen and rejected responses. This combination helps in evaluating the responses in a structured dialogue format.\n",
        "\n",
        "**Function definition**: Define a function `add_combined_columns` that takes an example (a single data point) and adds two new columns:\n",
        "- `prompt_chosen`: Combines the `prompt` with the `chosen` response in the same labeled format.\n",
        "- `prompt_rejected`: Combines the `prompt` with the `rejected` response in the same labeled format.\n",
        "\n",
        "**Apply function**: The `map` method is used to apply this function to each example in the training split of the data set. This method iterates over all the examples and modifies them in place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "274822a2-dfff-4433-b7ff-6699145d89ca"
      },
      "outputs": [],
      "source": [
        "# Define a function to combine 'prompt' with 'chosen' and 'rejected' responses\n",
        "def add_combined_columns(example):\n",
        "    # Combine 'prompt' with 'chosen' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
        "    example['prompt_chosen'] = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"chosen\"]\n",
        "\n",
        "    # Combine 'prompt' with 'rejected' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
        "    example['prompt_rejected'] = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"rejected\"]\n",
        "\n",
        "    # Return the modified example\n",
        "    return example\n",
        "\n",
        "# Apply the function to each example in the 'train' split of the dataset\n",
        "dataset['train'] = dataset['train'].map(add_combined_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2528e25-54c5-4a88-973c-cd5dd820ac55"
      },
      "source": [
        "When using pretrained transformers for classification tasks, understanding the maximum sequence length supported by the model is crucial, as pretrained transformers have a fixed maximum token length, for example, GPT-2 has 1024 tokens. Inputs longer than this are truncated, potentially losing important information. So a function is written to determine the max length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b6c2e3c-bfe5-44d8-b73a-4f62db79cd59"
      },
      "outputs": [],
      "source": [
        "get_max_len= lambda samples: max([len(sample) for sample in samples])\n",
        "get_max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "333cd995-76ac-4df8-9b80-e1ce1a2eedf9"
      },
      "outputs": [],
      "source": [
        "print(\"rejected samples length\",get_max_len(rejected_samples))\n",
        "print(\"chosen samples length\",get_max_len(chosen_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3abba7-5881-4a42-8509-3a4e9cf93a6f"
      },
      "source": [
        "Sometimes, you might want to identify samples shorter than a specified maximum length. This can be useful for filtering or handling special cases during preprocessing.\n",
        "\n",
        "The lambda function `find_short` takes a data set and a maximum length (`max_length`) as input. It uses a list comprehension to iterate over each example in the data set, enumerating both the index and the (chosen, rejected) pair. It zips `prompt_chosen` and `prompt_rejected` to pair each chosen response with its corresponding rejected response. For each pair, it checks if the length of either `chosen` or `rejected` is less than the specified `max_length`. If the condition is met, the index of that pair is included in the resulting list. The resulting list contains the index of all examples where either `prompt_chosen` or `prompt_rejected` is shorter than the specified `max_length`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fed596f6-bc7a-4748-9ec5-a7befdc0b570"
      },
      "outputs": [],
      "source": [
        "find_short = lambda dataset, max_length: [\n",
        "    i for i, (chosen, rejected) in enumerate(zip(dataset['prompt_chosen'], dataset['prompt_rejected']))\n",
        "    if len(chosen) < max_length or len(rejected) < max_length\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fdc5369-c1e1-43d0-a8ca-bba9902707b7"
      },
      "source": [
        "To ensure that your dataset only includes samples that meet the required length criteria, filter out any samples that are shorter than the specified `max_length`. This step is important for maintaining consistency in the input data for the model.\n",
        "\n",
        "Now, use the **GPT-2 model** for classification with a maximum length of 1024 tokens:\n",
        "\n",
        "1. Set the maximum length (`max_length`) to `1024`.\n",
        "2. Call the `find_short` function with the training dataset (`dataset['train']`) and `max_length` as arguments.\n",
        "3. This function will return indices of examples where either `prompt_chosen` or `prompt_rejected` is shorter than `max_length`.\n",
        "4. Use the resulting indices (`subset_indices`) to create a subset of the training dataset that only includes those examples.\n",
        "5. Update `dataset['train']` to this filtered subset.\n",
        "6. Optionally, print or return `subset_indices` for verification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9080b38-8257-4a7c-b64f-e96b9e22a9b2"
      },
      "outputs": [],
      "source": [
        "max_length=1024\n",
        "subset_indices=find_short (dataset['train'], max_length)\n",
        "dataset['train'] = dataset['train'].select(subset_indices)\n",
        "subset_indices[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fdb591b-591b-4cda-9e22-a1d725e1d199"
      },
      "source": [
        "The `preprocess_function` tokenizes the `prompt_chosen` and `prompt_rejected` keys, which are crucial for the **RewardTrainer**.\n",
        "\n",
        "- The `chosen` key represents the **preferred responses**.\n",
        "- The `rejected` key represents the **less preferred responses**.\n",
        "\n",
        "Tokenizing these keys allows the model to understand and process the differences between high-quality and low-quality responses.\n",
        "\n",
        "By providing both `chosen` and `rejected` inputs, the **RewardTrainer** learns to distinguish and prioritize better responses — a key step in training models to **follow instructions effectively**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4350bca-c45f-437c-aa62-ceb1b98d27f3"
      },
      "outputs": [],
      "source": [
        "# Define a preprocessing function to tokenize the 'prompt_chosen' and 'prompt_rejected' keys\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the 'prompt_chosen' text with truncation and padding to the maximum length\n",
        "    tokenized_chosen = tokenizer(examples['prompt_chosen'], truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize the 'prompt_rejected' text with truncation and padding to the maximum length\n",
        "    tokenized_rejected = tokenizer(examples['prompt_rejected'], truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "    # Return the tokenized inputs as a dictionary\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],  # Token IDs for 'chosen' responses\n",
        "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],  # Attention masks for 'chosen' responses\n",
        "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],  # Token IDs for 'rejected' responses\n",
        "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],  # Attention masks for 'rejected' responses\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ba8ead0-7570-47bc-9e1e-37f30125f7bd"
      },
      "source": [
        "The `input_ids_chosen` and `input_ids_rejected` fields contain the **token IDs** for the `chosen` and `rejected` responses, respectively. These are the numerical representations of the text that the model uses.\n",
        "\n",
        "The `attention_mask_chosen` and `attention_mask_rejected` fields contain the **attention masks** for the `chosen` and `rejected` responses. These masks indicate which tokens should be attended to (`1`) and which should be ignored (`0`).\n",
        "\n",
        "These fields are essential for the `RewardTrainer` because they provide the **tokenized inputs** and **attention masks** for both preferred and less preferred responses. By comparing the token IDs and attention patterns between the `chosen` and `rejected` responses, the `RewardTrainer` learns to distinguish high-quality from low-quality outputs.\n",
        "\n",
        "This helps the model improve its ability to **prioritize better responses** during instruction-following tasks.\n",
        "\n",
        "---\n",
        "\n",
        "You can apply the `reprocess_function` to one sample:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7de156f0-337e-41fe-96dc-75d6ecd69eae"
      },
      "outputs": [],
      "source": [
        "example=preprocess_function(dataset['train'][0])\n",
        "example.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca4736d-f1e3-4e7e-8097-cabe5b207f51"
      },
      "source": [
        "Now, create a dictionary with 'chosen' and 'rejected' samples from the training data set. This dictionary is created to make it easier to validate the model later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0668ef91-b220-4bea-8263-47513b63a47e"
      },
      "outputs": [],
      "source": [
        "train_str={'chosen': [sample for sample in dataset['train'] ['prompt_chosen']], 'rejected':[sample for sample in dataset['train'] ['prompt_rejected']]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa57d4b7-898d-4d7f-a843-53b26963dbb1"
      },
      "source": [
        "The code applies the `preprocess_function` to each example in the training dataset using the `map` method. This function tokenizes the `prompt_chosen` and `prompt_rejected` texts.\n",
        "\n",
        "- The `batched=True` parameter allows the function to process **multiple examples at once**, which improves efficiency.\n",
        "- The `remove_columns` parameter specifies a list of columns to be removed after processing:\n",
        "  - `prompt`\n",
        "  - `chosen`\n",
        "  - `rejected`\n",
        "  - `prompt_chosen`\n",
        "  - `prompt_rejected`\n",
        "\n",
        "Removing these columns ensures that only the **tokenized inputs** and **attention masks** generated by `preprocess_function` are retained. This simplifies the dataset structure and makes it more suitable for **model training and validation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7c09696-02bb-4c0f-bf27-882c1b218617"
      },
      "outputs": [],
      "source": [
        "dataset['train'] = dataset['train'].map(preprocess_function, batched=True, remove_columns=['prompt',\"chosen\", \"rejected\",'prompt_chosen', 'prompt_rejected'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61b7eb3a-11a4-4321-a7cb-c945a174300d"
      },
      "source": [
        "The only columns left are the tokens and masks indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a496ef0-65f9-4e4b-ab4c-5265cbc3d27a"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4a4ff9-cf46-488b-b2e7-7f94bd0eb372"
      },
      "source": [
        "Finally, split the data set into training and testing data set. FOr this purpose I can use all the data, but for the interest of tutotrail I will use 15% of the overall data for training and 5% for the test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cec7f5e4-ce62-4561-a2d8-0ded0bc44a79"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Step 1: Get 20% of the original dataset\n",
        "small_split = dataset['train'].train_test_split(test_size=0.20, seed=42)\n",
        "\n",
        "# Step 2: From that 20%, split 75% as train (i.e., 15% of original) and 25% as test (i.e., 5% of original)\n",
        "small_20_split = small_split['test'].train_test_split(test_size=0.25, seed=42)\n",
        "\n",
        "# Step 3: Create the new DatasetDict\n",
        "small_dataset_dict = DatasetDict({\n",
        "    'train': small_20_split['train'],  # 15% of original\n",
        "    'test': small_20_split['test']     # 5% of original\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWPmykmE1NX4"
      },
      "outputs": [],
      "source": [
        "small_dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fd17d49-9a89-4e0b-81d2-3562d67388bb"
      },
      "source": [
        "## LoRA Configuration\n",
        "\n",
        "Now that the training dataset is ready, it's time to begin training using a pretrained transformer model. However, to make training more efficient, it’s recommended to use a **LoRA (Low-Rank Adaptation)** configuration.\n",
        "\n",
        "### Define LoRA Configuration and Training Arguments\n",
        "\n",
        "First, initialize a `LoraConfig` for a **sequence classification** task using the `LoraConfig` class from the `peft` library. The configuration includes the following parameters:\n",
        "\n",
        "- **`task_type=TaskType.SEQ_CLS`**: Specifies the task type — in this case, sequence classification.\n",
        "- **`inference_mode=False`**: Indicates that the model is in training mode.\n",
        "- **`r=8`**: Sets the rank of the LoRA matrices.\n",
        "- **`lora_alpha=32`**: Defines the alpha scaling factor for the LoRA matrices.\n",
        "- **`lora_dropout=0.1`**: Applies dropout to the LoRA layers to help prevent overfitting.\n",
        "- **`target_modules=[\"attn.c_attn\", \"attn.c_proj\"]`**: Specifies the attention layers to be adapted using LoRA — `attn.c_attn` and `attn.c_proj`.\n",
        "\n",
        "This configuration allows **efficient fine-tuning** by updating only a subset of the model’s parameters, reducing computational cost while maintaining performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8c7452b-8502-42b8-a5cf-c4bf022af1ae"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"]  # Target attention layers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20e7b1cf-329b-4887-b474-28147ee08f33"
      },
      "source": [
        "## Training Arguments\n",
        "\n",
        "Define the training arguments using the `TrainingArguments` class from the `transformers` library. These arguments control various aspects of the training process:\n",
        "\n",
        "- **`per_device_train_batch_size=3`**: Sets the batch size per device (GPU/CPU) to 3.\n",
        "- **`num_train_epochs=3`**: Specifies that the model will be trained for 3 epochs.\n",
        "- **`gradient_accumulation_steps=8`**: Accumulates gradients over 8 steps before performing an update, effectively increasing the batch size.\n",
        "- **`learning_rate=1.41e-5`**: Sets the learning rate for the optimizer to `1.41e-5`.\n",
        "- **`output_dir=\"./model_output3\"`**: Defines the directory where model checkpoints and outputs will be saved.\n",
        "- **`logging_steps=10`**: Logs training progress every 10 steps.\n",
        "- **`eval_steps=500`**: Evaluates the model every 500 steps.\n",
        "- **`save_steps=500`**: Saves model checkpoints every 500 steps.\n",
        "- **`save_total_limit=2`**: Keeps only the latest 2 checkpoints, removing older ones to save space.\n",
        "\n",
        "These settings configure the training loop, covering aspects such as batch size, learning rate, logging frequency, evaluation schedule, and checkpointing strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2dfa68a-77b5-42ec-8b28-9a3109e4a370"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=3,  # Set to 3\n",
        "    num_train_epochs=3,  # Set to 3\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1.41e-5,\n",
        "    output_dir=\"./model_output3\",\n",
        "    logging_steps=10,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ce743f-bde1-4890-827e-776a5c851fc7"
      },
      "source": [
        "### RewardTrainer\n",
        "\n",
        "The `RewardTrainer` is a specialized trainer that is designed to train models with a reward signal. This is often used in reinforcement learning scenarios where the model learns to optimize for better responses. It is initialized with several parameters:\n",
        "\n",
        "- **model**: The model to be trained\n",
        "- **args**: The training arguments. Typically, an instance of `TrainingArguments`\n",
        "- **tokenizer**: The tokenizer used to process the text inputs\n",
        "- **train_dataset**: The training data set\n",
        "- **eval_dataset**: The evaluation data set\n",
        "- **peft_config**: The configuration for LoRA\n",
        "\n",
        "The `RewardTrainer` orchestrates the training process, handling tasks such as batching, optimization, evaluation, and saving model checkpoints. It is particularly useful for training models that need to learn from feedback signals, improving their ability to generate high-quality responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0487f47a-ef67-4af2-8b5f-df99a423bcef"
      },
      "outputs": [],
      "source": [
        "# Initialize RewardTrainer\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=small_dataset_dict['train'],\n",
        "    eval_dataset=small_dataset_dict['test'],\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e92a4e63-5bfb-4981-aa01-358a9b82d25d"
      },
      "source": [
        ">Note: You can safely ignore the above warning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df942822-2174-4f37-bcfe-d565c7ca22c3"
      },
      "source": [
        "### **Train, Save, and Evaluate the Model**\n",
        "\n",
        "Now that everything is set up, it's time to train the model using the `RewardTrainer`. This step will help the model learn how to better score responses based on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Train the Model**\n",
        "\n",
        "To start training, simply use:\n",
        "\n",
        "```python\n",
        "trainer.train()\n",
        "```\n",
        "This line starts the training process. The model looks at examples in the training dataset and updates its internal parameters to improve how it ranks good vs. bad responses.\n",
        "#### 2. Save the Trained Model\n",
        "Once training is finished, you can save the model to a folder for later use:\n",
        "```\n",
        "trainer.save_model(output_dir)\n",
        "```\n",
        "#### 3. Evaluate the Model\n",
        "To check how well the model performs, use the evaluate() method:\n",
        "```\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f3d8561-b63c-41b7-b6ef-a8dfa3e84fa7"
      },
      "outputs": [],
      "source": [
        "output_dir=\"./model_output3\"\n",
        "\n",
        "# # Train the model\n",
        "trainer.train()\n",
        "\n",
        "# # Save the model\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "# # Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n",
        "\n",
        "model.config.save_pretrained(\"./backup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc648c25-9d83-4650-ba7d-66ae3b4f3920"
      },
      "source": [
        "\n",
        "#6. Evaluating the model\n",
        "\n",
        "The `RewardTrainer` uses **pairwise comparison** to evaluate how well the model can distinguish between good and bad responses.\n",
        "\n",
        "- The model is shown two responses at a time:\n",
        "  - One labeled as **better** (`chosen`)\n",
        "  - The other labeled as **worse** (`rejected`)\n",
        "\n",
        "- It assigns a **score (logit)** to each response based on its training.\n",
        "\n",
        "- These scores reflect how much the model \"prefers\" each response.\n",
        "\n",
        "- The response with the **higher score** is selected as the better one.\n",
        "\n",
        "This process helps the model **learn to choose higher-quality answers**, improving its ability to follow instructions and generate useful outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5cb8e3-4a39-4992-b0c7-c080c5e1e752"
      },
      "source": [
        "Next, plot the loss. You can see it converges nicely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "753c1b89-0bdf-4df8-acce-a197307e3054"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "log_file = f\"model_output3/checkpoint-500/trainer_state.json\"\n",
        "\n",
        "# Read the log file\n",
        "with open(log_file, 'r') as f:\n",
        "    logs = json.load(f)\n",
        "\n",
        "# Extract training loss values\n",
        "steps = []\n",
        "losses = []\n",
        "for log in logs[\"log_history\"]:\n",
        "    if \"loss\" in log:\n",
        "        steps.append(log[\"step\"])\n",
        "        losses.append(log[\"loss\"])\n",
        "\n",
        "# Plot the training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Time\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35152074-8775-4c43-a320-1c4320cd2abe"
      },
      "source": [
        "The function ```predict_and_get_logits``` code tokenizes input text, performs efficient model inference on GPU (if available), and and outputs (logits).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make a prediction and get the logits\n",
        "def predict_and_get_logits(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Perform the forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the logits from the outputs\n",
        "    logits = outputs.logits.squeeze().item()  # Assuming binary classification and batch size of 1\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "iZGRcl-O6xlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```Let us calculate the logits score for the first choosen prompt```\n"
      ],
      "metadata": {
        "id": "ytO9kIJk7hTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a562053-aee1-411d-a8b2-312c702ddfef"
      },
      "outputs": [],
      "source": [
        "text1=train_str['chosen'][0]\n",
        "print(text1)\n",
        "print(\"logit score :\",predict_and_get_logits(text1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abd74b34-bfcb-42f7-ae84-d4b504c551b7"
      },
      "source": [
        "```Do the same for the rejected sample```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ae7f11-46cc-4256-bec8-3ac1ae5548f6"
      },
      "outputs": [],
      "source": [
        "text2=train_str['rejected'][0]\n",
        "print(text2)\n",
        "print(\"logit score :\",predict_and_get_logits(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6825f49f-7178-4e53-ad91-d5cebfdd30ce"
      },
      "outputs": [],
      "source": [
        "# Function to compare two texts\n",
        "def compare_texts(text1, text2):\n",
        "    logit1 = predict_and_get_logits(text1)\n",
        "    logit2 = predict_and_get_logits(text2)\n",
        "\n",
        "    if logit1 > logit2:\n",
        "        #print(\"selected---------\")\n",
        "        #print(text1, f\"score: {logit1}\")\n",
        "\n",
        "        return text1\n",
        "    else:\n",
        "        #print(\"selected---------\")\n",
        "        #print(text2,  f\"score: {logit2}\")\n",
        "\n",
        "        return text2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a856785d-5b25-4c5b-a911-daa5096f13b4"
      },
      "source": [
        "### Evaluating Model Performance with Pairwise Comparison\n",
        "\n",
        "To evaluate the model's performance, a **pairwise comparison** approach is applied on a subset of the dataset.\n",
        "\n",
        "- **N** is defined as the number of samples to evaluate.\n",
        "- A counter, `correct_selections`, is initialized to track how often the model correctly selects the preferred response.\n",
        "- The code iterates over the first **N pairs** of `chosen` and `rejected` responses from the training dataset (`train_str['chosen']` and `train_str['rejected']`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fb610a4-81d8-448c-acc0-4e8c1f0f119d"
      },
      "outputs": [],
      "source": [
        "# Define the number of samples to evaluate\n",
        "N = 1000\n",
        "\n",
        "# Initialize a counter for correct selections\n",
        "correct_selections = 0\n",
        "\n",
        "# Iterate over the first N pairs of chosen and rejected responses\n",
        "for chosen, rejected in zip(train_str['chosen'][0:N], train_str['rejected'][0:N]):\n",
        "\n",
        "    # Use the compare_texts function to determine which response is better\n",
        "    selected_text = compare_texts(chosen, rejected)\n",
        "\n",
        "    # Check if the selected text is the chosen response\n",
        "    if selected_text == chosen:\n",
        "        correct_selections += 1\n",
        "\n",
        "# Calculate the accuracy as the ratio of correct selections to the total number of samples\n",
        "accuracy = correct_selections / N\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "LjqSx_Zo9IrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idrWbEfA-QF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "ad577ec58a1de1656cc0c9c249f84e1d2a31ff41f7a9a1493f5ffe94939116ae"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}