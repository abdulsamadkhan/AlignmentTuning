{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/AlignmentTuning/blob/main/Instruction_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1d1fbca-f2ff-4ab3-ae55-c0dc13cccf7d"
      },
      "source": [
        "#  Instruction-Tuning with LLMs\n",
        "\n",
        "\n",
        "Instruction-based fine-tuning is a process where a pre-trained language model is further trained (fine-tuned) using instruction–response pairs. These pairs consist of a task description (instruction + context) and an appropriate answer (output). The idea is to make the model better at following natural language instructions—essentially training it to behave like a helpful assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In instruction tuning datasets like Alpaca, each example is typically a triplet (or close variant)\n",
        "```\n",
        "{\n",
        "  \"instruction\": \"...\",\n",
        "  \"input\": \"...\",\n",
        "  \"output\": \"...\"\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ko_fJXQPYGfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This format is used across different task and here is an example of some of the tasks.\n",
        "\n",
        "**Template: QA-style promp**\n",
        "```\n",
        "{\n",
        "  \"instruction\": \"Answer the question.\",\n",
        "  \"input\": \"What is the capital of France?\",\n",
        "  \"output\": \"The capital of France is Paris.\"\n",
        "}\n",
        "\n",
        "```\n",
        "**Template: Text Generation**\n",
        "```\n",
        "{\n",
        "  \"instruction\": \"Write a story about a dragon who learns to paint.\",\n",
        "  \"input\": \"\",\n",
        "  \"output\": \"Once upon a time in the hills of Eldara, a young dragon named Thalos discovered a set of magical brushes...\"\n",
        "}\n",
        "\n",
        "```\n",
        "**Template: Summarization**\n",
        "```\n",
        "{\n",
        "  \"instruction\": \"Summarize the following article.\",\n",
        "  \"input\": \"The recent advancements in AI have led to significant improvements in language understanding and generation...\",\n",
        "  \"output\": \"Recent AI advancements have greatly improved language understanding and generation.\"\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Conversation Template (Multi-turn Chat Style)**\n",
        "```\n",
        "{\n",
        "  \"instruction\": \"You are a helpful assistant.\",\n",
        "  \"input\": \"User: What's the weather like in Paris?\\nAssistant: It's currently sunny and 22°C.\\nUser: Great! Should I carry an umbrella?\",\n",
        "  \"output\": \"Assistant: It’s unlikely to rain today, but carrying a small umbrella just in case is always a good idea.\"\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LklqjLJsaLmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eff9c7e8-471c-43fb-9917-f38263b82491"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab, you will be able to:\n",
        "\n",
        " - Understand the various types of templates including instruction-response, question-answering, summarization, code generation, dialogue, data analysis, and explanation and their applications for fine-tuning large language models (LLMs).\n",
        " - Create and apply different templates to fine-tune LLMs for various tasks.\n",
        " - Format datasets based on the created templates to prepare them for effective model training\n",
        " - Perform instruction fine-tuning using Hugging Face libraries and tools\n",
        " - Apply Low-Rank Adaptation (LoRA) techniques to fine-tune LLMs efficiently\n",
        " - Configure and use the SFTTrainer for supervised fine-tuning of instruction-following models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec5cc1c1-7b77-4785-897d-9aeb42e19dc4"
      },
      "source": [
        "The concepts presented in this lab would apply to the other template formats as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31dbbbec-ecb7-459a-87c4-8227282b13de"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Setup](#Setup)\n",
        "    1. [Install required libraries](#Install-required-libraries)\n",
        "    2. [Import required libraries](#Import-required-libraries)\n",
        "    3. [Define the device](#Define-the-device)\n",
        "2. [Dataset description](#Dataset-description)\n",
        "3. [Model and tokenizer](#Model-and-tokenizer)\n",
        "4. [Preprocessing the data](#Preprocessing-the-data)\n",
        "5. [Test the base model](#Test-the-base-model)\n",
        "    1. [BLEU score](#BLEU-score)\n",
        "6. [Perform instruction fine-tuning with LoRA](#Perform-instruction-fine-tuning-with-LoRA)\n",
        "7. [Exercises](#Exercises)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a0df701-9541-41b0-90d9-52212275948a"
      },
      "source": [
        "# Setup\n",
        "\n",
        "### Install required libraries\n",
        "\n",
        "For this lab, use the following libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae359601-0136-4fd3-8de9-b762753cef1d"
      },
      "outputs": [],
      "source": [
        "!pip install  datasets\n",
        "!pip install  trl\n",
        "!pip install transformers\n",
        "!pip install  peft\n",
        "!pip install  tqdm\n",
        "!pip install  numpy\n",
        "!pip install  pandas\n",
        "!pip install  matplotlib\n",
        "!pip install  seaborn\n",
        "!pip install  scikit-learn\n",
        "!pip install  sacrebleu\n",
        "!pip install  evaluate"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b692a2-4f7d-4d76-a11d-49f0fe6be003"
      },
      "source": [
        "### Import required libraries\n",
        "\n",
        "The following code imports the required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95f74e49-7538-48e3-a940-30d5354f907e"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from urllib.request import urlopen\n",
        "import io"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec1e702-7bf2-45f7-bac5-38859436cfa7"
      },
      "source": [
        "### Define the device\n",
        "\n",
        "The below code will set your device to 'cuda' if your device is compatible with GPU, otherwise, you can use 'cpu'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51e02902-9804-4e41-a5d4-8d49fca73461"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0406626e-feca-4770-b922-06ef67cd4493"
      },
      "source": [
        "# Dataset description\n",
        "\n",
        "We will use the CodeAlpaca 20k dataset, a programming code dataset. This data is available [here](https://github.com/sahil280114/codealpaca?tab=readme-ov-file#data-release). The CodeAlpaca dataset contains the following elements:\n",
        "\n",
        "\n",
        "- `instruction`: **str**, describes the task the model should perform. Each of the 20K instructions is unique.\n",
        "- `input`: **str**, optional context or input for the task.  Around 40% of the examples have an input.\n",
        "- `output`: **str**, the answer to the instruction as generated by text-davinci-003.\n",
        "\n",
        "The following code block downloads the CodeAlpaca-20k dataset as a `json` file:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d645484a-3469-42d7-b306-ed49bd8a7911"
      },
      "outputs": [],
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ced33fd-43e2-43de-a5e9-b18260a22aae"
      },
      "source": [
        "Next we will load the dataset as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ae32a2-9aa6-498f-a4ef-75c16fe9803b"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"CodeAlpaca-20k.json\", split=\"train\")\n",
        "dataset"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3c7e1ee-581e-4b33-9b3a-af4932030d0a"
      },
      "source": [
        "Each example in the dataset looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89242eb8-dd68-4367-a4e4-fb9ae90d9ad0"
      },
      "outputs": [],
      "source": [
        "dataset[100]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83d3137-3d64-49d6-bab6-fd9fa78e8cf5"
      },
      "source": [
        "To simplify, let's focus only on examples without any `input`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6639ff90-b4d4-43da-96c1-7d3c10412d84"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.filter(lambda example: example[\"input\"] == '')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d6f7ecb-7b35-4101-8985-ee4b10363643"
      },
      "source": [
        "This line shuffles the dataset with a random seed of 42 for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ba53d0-9736-4615-94db-cbabeda88f65"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ad2a150-4f91-4fa8-b117-d63ae3463091"
      },
      "outputs": [],
      "source": [
        "dataset"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ef623e-9000-4bcd-a57a-db58815b6abc"
      },
      "source": [
        "The CodeAlpaca 20k dataset includes training and test sets. You can split the original training data by allocating 80% to the training set and 20% to the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ccd4265-43cd-4733-a40f-e4e26ee70592"
      },
      "outputs": [],
      "source": [
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset_split['train']\n",
        "test_dataset = dataset_split['test']\n",
        "dataset_split"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cff22c5-df09-4651-965c-16f157c384f2"
      },
      "outputs": [],
      "source": [
        "# Select a small set of data for If you are having issue with computation. Here we are using only 10% of the data\n",
        "\n",
        "tiny_test_dataset=test_dataset.select(range(int(len(test_dataset)*.1)))\n",
        "tiny_train_dataset=train_dataset.select(range(int(len(train_dataset)*.1)))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ca2f05c-42ae-414a-8408-d070b19388bf"
      },
      "source": [
        "# Model and tokenizer\n",
        "We will load the model, [`opt-350m`](https://huggingface.co/facebook/opt-350m)  from Facebook. A description of this OpenSource model was published [here](https://arxiv.org/abs/2205.01068).\n",
        "The below lines load the base model from Hugging Face:\n",
        "**Note:** you can also use (EleutherAI/gpt-neo-125m) which is more lighter model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e9de4b5-b432-4db1-a7dd-63de96bb2ba3"
      },
      "outputs": [],
      "source": [
        "# Base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba9368ec-f21e-489f-88ca-fc0c0d517442"
      },
      "source": [
        "The below code line will load the tokenizer for the model. just note that each model has it's specific tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3853812-0997-4812-b798-7aa566577f77"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0e7b2a6-8cb4-46f0-a8a9-34bcf3db052c"
      },
      "source": [
        "Let's identify the End of Sentence (EOS) token, a special tokenizer token that signals the model to stop generating additional tokens once encountered:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26835ee8-3dcf-4b1d-a49d-c38a6ab16b10"
      },
      "outputs": [],
      "source": [
        "tokenizer.eos_token"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2c3bfa4-dcb2-45af-830e-1de1c0fa7c6b"
      },
      "source": [
        "# Preprocessing the data\n",
        "\n",
        "We will transform our data into the format called prompt which we can send to the model To perform the fine-tuning, for this task we will create a functions that converts all the examples into prompts.\n",
        "\n",
        "\n",
        "```\n",
        "### Instruction:\n",
        "Create an array of 4 elements\n",
        "\n",
        "### Response:\n",
        "A = [1,2,3,4]</s>\n",
        "```\n",
        "\n",
        "_**Note:**_\n",
        "Introducing the `</s>` end of sentence token at the end of the text informs the model to stop generating text beyond this point.\n",
        "\n",
        "The `formatting_prompts_func` function takes a dataset as input. For every element in the dataset format, the instruction and the output into a template using the format:\n",
        "\n",
        "\n",
        "The `formatting_prompts_func_no_response` function behaves similarly to the `formatting_prompts_func` except the response is not included.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b2142f8-1835-4b50-9768-44e753299ac7"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n{mydataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_no_response(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabf903e-320a-4906-8454-ca30d7e2d7c0"
      },
      "source": [
        "# Processing Prompts for Fine-Tuning with CodeAlpaca\n",
        "\n",
        "In this tutorial section, we'll dive into generating and processing prompts for fine-tuning a model using the CodeAlpaca dataset. The provided code block generates three essential components:\n",
        "\n",
        "- **`instructions`**: The initial part of the prompt, including only the instruction (and optional input) without the response.\n",
        "- **`instructions_with_responses`**: The complete prompt, encompassing the instruction, the response, and the end-of-sequence (`eos`) token.\n",
        "- **`expected_outputs`**: The response portion of the prompt, extracted from `instructions_with_responses` by isolating the text between the `instructions` and the `eos` token.\n",
        "\n",
        "## Extracting `expected_outputs`\n",
        "\n",
        "To isolate the `expected_outputs`, follow these steps:\n",
        "\n",
        "1. **Tokenize the Inputs**: Use the tokenizer to convert both `instructions` and `instructions_with_responses` into token IDs.\n",
        "2. **Count Instruction Tokens**: Calculate the number of tokens in `instructions`.\n",
        "3. **Trim the Full Prompt**: Remove the first `N` tokens from the tokenized `instructions_with_responses`, where `N` is the token count of `instructions`. This eliminates the instruction part.\n",
        "4. **Remove the EOS Token**: Discard the final token from the remaining `instructions_with_responses` tokens, as it represents the `eos` token.\n",
        "5. **Decode the Result**: Convert the remaining token IDs back into text using the tokenizer to obtain the `expected_output`, which is the response text.\n",
        "\n",
        "This approach ensures that `expected_outputs` contains only the response, which is critical for tasks like computing loss during fine-tuning, where the focus is on the model's generated output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "892d8d42-f92c-4734-a855-3fba6703372f"
      },
      "outputs": [],
      "source": [
        "expected_outputs = []\n",
        "instructions_with_responses = formatting_prompts_func(test_dataset)\n",
        "instructions = formatting_prompts_func_no_response(test_dataset)\n",
        "for i in tqdm(range(len(instructions_with_responses))):\n",
        "    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors=\"pt\", max_length=1024, truncation=True, padding=False)\n",
        "    tokenized_instruction = tokenizer(instructions[i], return_tensors=\"pt\")\n",
        "    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)\n",
        "    expected_outputs.append(expected_output)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae98970a-017c-4be3-8b7c-52add4fd44f8"
      },
      "source": [
        "Let's look at the example to view what `instructions` include, `instructions_with_responses`, and `expected_outputs`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7bf208e-1ffe-4b17-92a2-cbad38261523"
      },
      "outputs": [],
      "source": [
        "print('############## instructions ##############\\n' + instructions[0])\n",
        "print('############## instructions_with_responses ##############\\n' + instructions_with_responses[0])\n",
        "print('\\n############## expected_outputs ##############' + expected_outputs[0])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d6c8f01-0b8b-486f-8d20-ee9fc56b7480"
      },
      "source": [
        "Instead of keeping the instructions as-is, it's beneficial to convert the `instructions` list into a `torch` `Dataset`. The following code defines a class called `ListDataset` that inherits from `Dataset` and creates a `torch` `Dataset` from a list. This class is then used to generate a `Dataset` object from `instructions`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "173e3ea7-083a-462d-ba15-41ff24f5917e"
      },
      "outputs": [],
      "source": [
        "class ListDataset(Dataset):\n",
        "    def __init__(self, original_list):\n",
        "        self.original_list = original_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_list)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.original_list[i]\n",
        "\n",
        "instructions_torch = ListDataset(instructions)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4451831a-101f-43cc-847b-fe7419b77865"
      },
      "outputs": [],
      "source": [
        "instructions_torch[2]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efca3a45-12f3-46e5-90e3-45f1fb1bddeb"
      },
      "source": [
        "# Test the base model\n",
        "\n",
        "Let's evaluate the performance of the base model without fine-tuning. This involves generating responses using the non-fine-tuned model to assess its baseline capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8b3db7a-4595-4e2a-8cd9-767123a30120"
      },
      "source": [
        "The following code creates a text generation pipeline using the `pipeline` class from the `transformers` library. This pipeline enables text generation by leveraging a specified model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b9ed607-cf73-4a4d-b1ba-624757282fc1"
      },
      "outputs": [],
      "source": [
        "gen_pipeline = pipeline(\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device,\n",
        "                        batch_size=2,\n",
        "                        max_length=50,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_full_text=False)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcdb53fd-fa6f-4281-85de-f8fbe8b7a001"
      },
      "source": [
        "**_Note:_** The text generation pipeline typically outputs both the instructions and the responses together. To focus exclusively on the model's performance by analyzing only the generated responses, omit the instructions from the output. You can do this by configuring the pipeline with `return_full_text=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6785e569-18f4-4157-b1f7-76666c14775a"
      },
      "source": [
        "The below code leverages the pre-defined generation pipeline to generate outputs using the model.\n",
        "\n",
        "**_Note:_** The code is commented out because it may take a long time for CPU. Instead of generating the raw tokens here, you can load output from this model later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30433db5-c588-449e-b4e1-19ffb0263dac"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    pipeline_iterator= gen_pipeline(instructions_torch,\n",
        "                                    max_length=200,\n",
        "                                    num_beams=5,\n",
        "                                    early_stopping=True,)\n",
        "\n",
        "generated_outputs_base = []\n",
        "for text in pipeline_iterator:\n",
        "    generated_outputs_base.append(text[0][\"generated_text\"])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04096b0-c3be-4ed6-961e-a922ea47ffd4"
      },
      "source": [
        "Let's look at the sample responses generated by the base model and the expected responses from the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94b05d01-9471-4b5b-b2dd-ee1f96906d7c"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('--------------------')\n",
        "    print('----- Instruction '+ str(i+1) +': ')\n",
        "    print(instructions[i])\n",
        "    print('\\n\\n')\n",
        "    print('----- Expected response '+ str(i+1) +': ')\n",
        "    print(expected_outputs[i])\n",
        "    print('\\n\\n')\n",
        "    print('----- Generated response '+ str(i+1) +': ')\n",
        "    print(generated_outputs_base[i])\n",
        "    print('\\n\\n')\n",
        "    print('--------------------')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da41a838-b471-4293-ac37-1f158621d324"
      },
      "source": [
        "**Note:**\n",
        " You'have observed that outputs from the non-fine-tuned model often lack quality and relevance. Moreover, these responses tend to be overly verbose, repeating content unnecessarily until they hit the maximum token limit.\n",
        "\n",
        "Instruction tuning provides an effective solution to these challenges. Through fine-tuning, you can achieve two significant improvements:\n",
        "\n",
        "1. **Improved Response Quality**: The instruction-tuned model will produce more accurate, relevant, and meaningful responses that align closely with the provided instructions.\n",
        "2. **Controlled Response Length**: By incorporating the `<s>` end-of-sequence (EOS) token in the training outputs, you train the model to stop generating once the response is complete, avoiding endless or repetitive text.\n",
        "\n",
        "These enhancements will greatly elevate the model's performance, as you'll discover in the subsequent sections of this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed780c77-0823-466a-ad7e-d7a0fb9ea428"
      },
      "source": [
        "## BLEU score\n",
        "\n",
        "In this tutorial, we'll employ the BLEU metric to evaluate how closely the model's generated responses match the expected responses in the test environment. BLEU scores, which range from 0 to 1 (or 0 to 100 in the implementation used here), reflect the similarity between the generated and expected outputs, with higher scores indicating a better match.\n",
        "\n",
        "Note: While BLEU is a valuable metric for assessing alignment between model outputs and expected responses, it may not always be the optimal choice for instruction fine-tuning. Nevertheless, it provides a useful measure of how well the model's outputs align with the desired results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eac4935a-13d5-4b68-abbf-1217d563648b"
      },
      "outputs": [],
      "source": [
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "results_base = sacrebleu.compute(predictions=generated_outputs_base,\n",
        "                                 references=expected_outputs)\n",
        "\n",
        "print(list(results_base.keys()))\n",
        "print(round(results_base[\"score\"], 1))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e1d6f6-453b-4ae3-a537-0c2426d5e1b5"
      },
      "source": [
        "The low SacreBLEU score indicates that there is very little alignment between the base model's generated responses and the expected responses for the examples in the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64b1285-2f69-4202-9bf6-4d35e62280be"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e114090d-b325-488c-9cd3-a29b5765593b"
      },
      "source": [
        "## Perform instruction fine-tuning with LoRA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80835794-75ba-4df6-9831-922c93bffd0c"
      },
      "source": [
        "To optimize fine-tuning, we'll employ **Low-Rank Adaptation (LoRA)**, a highly efficient **parameter-efficient fine-tuning (PEFT)** method. Start by converting the model into a LoRA-compatible PEFT model. First, create a `LoraConfig` object using the `peft` library, defining critical LoRA parameters like the **LoRA rank** (e.g., `r=16`) and **target modules** (e.g., attention layers such as `q_proj` and `v_proj`). Next, apply this configuration to the model with the `get_peft_model()` function, transforming it into a LoRA-adapted model primed for resource-efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc17fdba-c750-49cb-a6a9-003828be8209"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,  # Low-rank dimension\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining Instruction Fine-Tuning: Focusing on the Response\n",
        "\n",
        "When fine-tuning a language model using instructions (often with tools like `SFTTrainer`), the model typically learns to generate the **entire sequence**: both the original instruction prompt *and* the desired response.\n",
        "\n",
        "However, our primary goal is usually to enhance the model's ability to generate high-quality **responses**. The model's reproduction of the *instruction* isn't the main focus for optimization or evaluation in this context.\n",
        "\n",
        "Here's how this distinction is handled in practice:\n",
        "\n",
        "#### 1. During Evaluation (e.g., Calculating BLEU Score)\n",
        "\n",
        "* **The Challenge:** The model's output includes both the instruction and the response. Calculating metrics like BLEU on the full output would misleadingly inflate scores based on the regenerated instruction, not just the response quality.\n",
        "* **The Method:** Isolate the response.\n",
        "    1.  Determine the token length of the original input instruction.\n",
        "    2.  Take the model's complete generated sequence (instruction + response).\n",
        "    3.  Discard the initial tokens equal to the instruction's length.\n",
        "* **Example:**\n",
        "    * Input Instruction Length: 10 tokens\n",
        "    * Model's Total Output Length: 14 tokens\n",
        "    * Tokens used for BLEU calculation: The final 4 tokens (14 - 10 = 4), representing the response.\n",
        "\n",
        "#### 2. During Training (Calculating the Loss)\n",
        "\n",
        "* **The Challenge:** We need the model's learning (guided by the loss function) to concentrate *only* on the response part. Penalizing the model for how it reconstructs the instruction is counterproductive. Simply removing tokens isn't suitable for loss calculation during training.\n",
        "* **The Method:** Use **masking**.\n",
        "    1.  Identify the target labels corresponding to the *instruction* tokens in the sequence.\n",
        "    2.  Replace these specific labels with a special value, commonly `-100`.\n",
        "* **Why `-100`?** Standard loss functions (e.g., PyTorch's `CrossEntropyLoss`) are designed to specifically ignore sequence positions labeled with `-100`. No loss is computed for these masked tokens.\n",
        "* **The Outcome:** Only the tokens corresponding to the actual **response** contribute to the loss value. This ensures that the model's parameter updates (learning) are driven solely by its performance on generating the correct response, focusing the fine-tuning effectively."
      ],
      "metadata": {
        "id": "ESX6WnZUDJ91"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e447052-504d-4f85-afeb-5c80600497d6"
      },
      "source": [
        "\n",
        "\n",
        "You can create such a masking manually by defining your own function. However, it is easier to instead use the `DataCollatorForCompletionOnlyLM` class from `trl`:\n",
        "\n",
        "**response_template` as a Marker:** The string \"### Response:\\n\" acts as a specific marker within the input sequences.\n",
        "\n",
        "**Data Collator's Role:** The DataCollatorForCompletionOnlyLM is designed to identify this marker in your input data. Its primary goal is to prepare the data in a way that the loss function during training is only calculated on the tokens that come after this marker.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edb4c0a6-70f9-40ce-a8b2-1225210c8b27"
      },
      "outputs": [],
      "source": [
        "response_template = \"### Response:\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "020499bf-a9fc-4495-ad50-aba665827b78"
      },
      "source": [
        "\n",
        "\n",
        "To perform the training, first configure our `SFTTrainer`, and create the `SFTTrainer` object by passing to the `collator`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a96ac6c-999e-40b9-ad8c-169efbb5b12b"
      },
      "outputs": [],
      "source": [
        "training_args = SFTConfig(\n",
        "    output_dir=\"/tmp\",\n",
        "    num_train_epochs=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=2,  # Reduce batch size\n",
        "    per_device_eval_batch_size=2,  # Reduce batch size\n",
        "    max_seq_length=1024,\n",
        "    do_eval=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    args=training_args,\n",
        "    data_collator=collator,\n",
        ")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfc59810-23f8-48d4-a1a5-3cdf22719c4e"
      },
      "source": [
        "Please ignore the above warning.\n",
        "The below comments, runs the trainer, because this would take a long time on the CPU. Therefore, let's not run the trainer here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6adf3712-1ee6-42f4-b2b9-632abb49893a"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95fab575-9c8d-428c-b224-1507e050d5b5"
      },
      "source": [
        "To track the trainer's progress, the trainer object records its state at each training step. You can access this history using the commented-out line below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "100dacb3-1554-4920-8904-f0af01f55c1a"
      },
      "outputs": [],
      "source": [
        "log_history_lora = trainer.state.log_history"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917163c5-da15-4278-923c-2fbf24cd0465"
      },
      "source": [
        "You can plot the training loss for each training step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8764f8a-c8cc-4bd1-9460-71045354f650"
      },
      "outputs": [],
      "source": [
        "train_loss = [log[\"loss\"] for log in log_history_lora if \"loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c09f166-f38f-4723-9f95-c484ef314660"
      },
      "source": [
        "save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007f2261-d46a-4b9f-89b8-3436b478ba06"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./instruction_tuning_final_model_lora\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796f92ac-3549-4d25-a920-3f401146da63"
      },
      "source": [
        "Let's redefine the text generation pipeline because the model has been changed to the LoRA model. Ignore the warning for the `PeftModelForCausalLM` not being supported for `text-generation`. However, if the PEFT model is supported, the warning is erroneous.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ad1b8d-bc7a-4118-b723-576e53dcd37a"
      },
      "outputs": [],
      "source": [
        "gen_pipeline = pipeline(\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device,\n",
        "                        batch_size=2,\n",
        "                        max_length=50,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_full_text=False)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a011b97c-c41c-440f-8807-dfc3091ac8ec"
      },
      "source": [
        "The below code generates tokens with the pipeline using the instruction fine-tuned model. Only three records of data are used for demonstration  because generating text is time consuming on CPU:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33538946-070e-4dab-bb19-a2b9ae268772"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    pipeline_iterator= gen_pipeline(instructions_torch,\n",
        "                                max_length=50,\n",
        "                                num_beams=5,\n",
        "                                early_stopping=True,)\n",
        "generated_outputs_lora = []\n",
        "for text in pipeline_iterator:\n",
        "    generated_outputs_lora.append(text[0][\"generated_text\"])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb199f1c-517a-4c93-a0b1-8a9e1decc0f5"
      },
      "outputs": [],
      "source": [
        "generated_outputs_lora[2]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf08765-ada4-4774-9c28-1d633f82efa5"
      },
      "source": [
        "Let's have a look at some of the responses from the instruction fine-tuned model and the expected responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b082fad-31fb-4588-82e5-fe65302c4eb8"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('--------------------')\n",
        "    print('----- Instruction '+ str(i+1) +': ')\n",
        "    print(instructions[i])\n",
        "    print('\\n\\n')\n",
        "    print('----- Expected response '+ str(i+1) +': ')\n",
        "    print(expected_outputs[i])\n",
        "    print('\\n\\n')\n",
        "    print('----- Generated response '+ str(i+1) +': ')\n",
        "    print(generated_outputs_lora[i])\n",
        "    print('\\n\\n')\n",
        "    print('--------------------')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efcfaa89-c3ed-4318-b103-1829a696f2e9"
      },
      "source": [
        "Compared to the base model, you can see that the responses are much better. Additionally, the responses don't extend until the maximum number of tokens are generated.\n",
        "\n",
        "To confirm the responses generated by the instruction fine-tuned model align better with the expected output, let's calculate the SacreBLEU score:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ce3412c-dbb7-490f-97a1-2dd892b2cb13"
      },
      "outputs": [],
      "source": [
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "results_lora = sacrebleu.compute(predictions=generated_outputs_lora,\n",
        "                                 references=expected_outputs)\n",
        "print(list(results_lora.keys()))\n",
        "print(round(results_lora[\"score\"], 1))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781b7387-456e-4124-8cea-7da643ded57b"
      },
      "source": [
        "You can see that the fine-tuned model achieves SacreBLEU score which is significantly better than the one achieved by the base model.\n",
        "\n",
        "Let's conclude. The instruction fine-tuned model generates responses that align much better with the expected responses in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b9240cc-bd0f-4612-b6f7-9fce33064983"
      },
      "source": [
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "193b491dbb3af9428b95c141e897c5ad06433b61b2a80b8b5bbc25b3b509c143",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}