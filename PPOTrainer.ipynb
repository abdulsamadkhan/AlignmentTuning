{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/AlignmentTuning/blob/main/PPOTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adf2d03-2220-4716-ad4f-81f11cbd9793"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback Using PPO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cd32eb5-9406-4aee-a232-909aa7a489fd"
      },
      "source": [
        "# Training an RL Agent with PPO and Sentiment Analysis\n",
        "\n",
        "Proximal Policy Optimization (PPO), developed by OpenAI, is a highly effective reinforcement learning (RL) algorithm known for balancing simplicity and performance. It optimizes policies directly while ensuring stable updates to maintain training reliability, making it a top choice for RL tasks.\n",
        "\n",
        "This tutorial guides you through training an RL agent using PPO, with a focus on sentiment analysis. You'll utilize the IMDb dataset, a vast collection of movie reviews, to train your model. By the end, you'll understand how to implement PPO for RL, gaining practical skills to apply to other datasets and problems.\n",
        "\n",
        "This tutorial is based on [Hugging Face's example code: `Tune GPT2 to generate positive reviews`](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4181cd64-b8a7-416d-bde5-6489a8f697ed"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Setup](#setup)\n",
        "    - [Installing required libraries](#installing-required-libraries)\n",
        "    - [Importing required libraries](#importing-required-libraries)\n",
        "    - [Defining helper functions](#defining-helper-functions)\n",
        "2. [Initializing the PPO configuration, model, and tokenizer](#initializing-the-ppo-configuration-model-and-tokenizer)\n",
        "    - [AutoModelForCausalLMWithValueHead Overview](#automodelforcausallmwithvaluehead-overview)\n",
        "3. [Dataset and processing of Dataset](#dataset-and-dataset-tokenization)\n",
        "4. [Collator function](#collator-function)\n",
        "5. [Initialize PPOTrainer](#initialize-ppotrainer)\n",
        "6. [Reward function](#reward-function)\n",
        "7. [Generating responses using PPO](#generating-responses-using-ppo)\n",
        "   1. [Tokenizing and preparing the input batch](#tokenizing-and-preparing-the-input-batch)\n",
        "   2. [Scoring function](#scoring-function)\n",
        "   3. [Proximal policy optimization](#proximal-policy-optimization)\n",
        "8. [Plotting PPO training loss and mean](#plotting-ppo-training-loss-and-mean)\n",
        "9. [Generating and analyzing text with PPO and reference models](#generating-and-analyzing-text-with-ppo-and-reference-models)\n",
        "10. [Comparing PPO and reference models](#comparing-ppo-and-reference-models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a429bff-2816-42db-b3f1-c6c484ddf0af"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57671cb8-f0da-4373-8ea7-c9efec203cf9"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f69c748-a664-4342-af0f-d0aaa04dd4c7"
      },
      "source": [
        "### Installing required libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca81f114-14d5-47ac-8e8b-e7bac89f404e"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext\n",
        "!pip install  datasets==3.2.0\n",
        "!pip install  trl==0.11\n",
        "!pip install transformers==4.43.4\n",
        "!pip install  nltk==3.9.1 rouge_score==0.1.2\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c33a8ef2-88df-4839-affe-b622b8cad705"
      },
      "source": [
        "### Importing required libraries\n",
        "\n",
        "_It is recommended that you import all required libraries in one place (here):_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf75973-c4fb-4ef0-b791-f515d59e49bb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer,AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler\n",
        "import os\n",
        "\n",
        "import tarfile\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71f4e10f-91d6-464e-b3ed-de02fb759f5e"
      },
      "source": [
        "## Defining helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91817c5c-5882-4bc1-b023-c2b1c11275ca"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, file_path):\n",
        "    \"\"\"\n",
        "    Save a dictionary to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The dictionary to save.\n",
        "        file_path (str): The path to the JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(f\"Data successfully saved to {file_path}\")\n",
        "\n",
        "\n",
        "def load_from_json(file_path):\n",
        "    \"\"\"\n",
        "    Load data from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: The data loaded from the JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return data\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function adds padding to a 1D PyTorch tensor so that its length matches a given target length.\n",
        "If padding is needed, it appends the specified `pad_token_id`; otherwise, it returns the original tensor.\n"
      ],
      "metadata": {
        "id": "pV4azcZH88nR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c74b103-ed69-43bf-b692-1e5b71e90182"
      },
      "outputs": [],
      "source": [
        "def pad_sequence_to_length(tensor, length, pad_token_id):\n",
        "    padding_length = length - tensor.size(0)\n",
        "    if padding_length > 0:\n",
        "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long, device=tensor.device)\n",
        "        return torch.cat((tensor, padding))\n",
        "    return tensor\n",
        "\n",
        "\n",
        "t = torch.tensor([1, 2, 3])\n",
        "padded = pad_sequence_to_length(t, 5, pad_token_id=0)\n",
        "print(padded)\n",
        "\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Pads each tensor in the list to the maximum length using `pad_token_id`, then ensures the total number of tensors equals `batch_size`.\n",
        "# If there are fewer tensors, it adds padding-only tensors; if more, it truncates to match the batch size.\n"
      ],
      "metadata": {
        "id": "TxWQ0a2D9Jmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pads each tensor in the list to the maximum length using `pad_token_id`, then ensures the total number of tensors equals `batch_size`.\n",
        " If there are fewer tensors, it adds padding-only tensors; if more, it truncates to match the batch size.\n"
      ],
      "metadata": {
        "id": "yzWaV3C59L-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_list_to_batch_size(tensors, batch_size, pad_token_id):\n",
        "    max_length = max(t.size(0) for t in tensors)\n",
        "    padded_tensors = [pad_sequence_to_length(t, max_length, pad_token_id) for t in tensors]\n",
        "\n",
        "    # Add additional padding-only tensors if needed\n",
        "    while len(padded_tensors) < batch_size:\n",
        "        padded_tensors.append(torch.full((max_length,), pad_token_id, dtype=torch.long, device=tensors[0].device))\n",
        "\n",
        "    return padded_tensors[:batch_size]\n",
        "tensors = [torch.tensor([1, 2,3 ]), torch.tensor([3])]\n",
        "batch = pad_list_to_batch_size(tensors, batch_size=4, pad_token_id=0)\n",
        "for b in batch:\n",
        "    print(b)"
      ],
      "metadata": {
        "id": "Ez2PxkMQn3C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3af38fa5-edde-466f-9e7b-26fe04b0838c"
      },
      "outputs": [],
      "source": [
        "def print_ppo_stats(stats, related_to_objective=False):\n",
        "    print(\"PPO Training Statistics\\n\")\n",
        "\n",
        "    if related_to_objective:\n",
        "        print(\"Objective Statistics:\")\n",
        "        print(f\"  KL Divergence (objective/kl): {stats['objective/kl']}\")\n",
        "        print(f\"  KL Coefficient (objective/kl_coef): {stats['objective/kl_coef']}\")\n",
        "        print(f\"  Entropy (objective/entropy): {stats['objective/entropy']}\\n\")\n",
        "\n",
        "        print(\"PPO Losses (Related to Minimizing Objective Function):\")\n",
        "        print(f\"  Policy Loss (ppo/loss/policy): {stats['ppo/loss/policy']}\")\n",
        "        print(f\"  Value Loss (ppo/loss/value): {stats['ppo/loss/value']}\")\n",
        "        print(f\"  Total Loss (ppo/loss/total): {stats['ppo/loss/total']}\\n\")\n",
        "\n",
        "        print(\"PPO Policy Statistics:\")\n",
        "        print(f\"  Policy Entropy (ppo/policy/entropy): {stats['ppo/policy/entropy']}\")\n",
        "        print(f\"  Approx KL (ppo/policy/approxkl): {stats['ppo/policy/approxkl']}\")\n",
        "        print(f\"  Clip Fraction (ppo/policy/clipfrac): {stats['ppo/policy/clipfrac']}\\n\")\n",
        "    else:\n",
        "        print(\"Reward and Value Function Estimation:\")\n",
        "        print(f\"  Mean Non-Score Reward (ppo/mean_non_score_reward): {stats['ppo/mean_non_score_reward']}\")\n",
        "        print(f\"  Mean Scores (ppo/mean_scores): {stats['ppo/mean_scores']}\")\n",
        "        print(f\"  Std Scores (ppo/std_scores): {stats['ppo/std_scores']}\")\n",
        "        print(f\"  Value Prediction (ppo/val/vpred): {stats['ppo/val/vpred']}\")\n",
        "        print(f\"  Value Prediction Error (ppo/val/error): {stats['ppo/val/error']}\")\n",
        "        print(f\"  Value Prediction Variance (ppo/val/var): {stats['ppo/val/var']}\")\n",
        "        print(f\"  Value Prediction Mean (ppo/val/mean): {stats['ppo/val/mean']}\")\n",
        "        print(f\"  Explained Variance (ppo/val/var_explained): {stats['ppo/val/var_explained']}\\n\")\n",
        "\n",
        "    print(\"Token Lengths:\")\n",
        "    print(f\"  Queries Length Mean (tokens/queries_len_mean): {stats['tokens/queries_len_mean']}\")\n",
        "    print(f\"  Responses Length Mean (tokens/responses_len_mean): {stats['tokens/responses_len_mean']}\\n\")\n",
        "\n",
        "    print(\"Time Statistics:\")\n",
        "    print(f\"  Total Time (time/ppo/total): {stats['time/ppo/total']} seconds\\n\")\n",
        "\n",
        "# Example usage with the provided stats and the flag"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d527f7f9-7444-468e-824b-021ee624b72e"
      },
      "source": [
        "#2. Initializing the PPO configuration, model, and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bb1f01-9b7b-4c68-b7b3-af80247b83c4"
      },
      "source": [
        "The `PPOConfig` class is used to specify the model and learning rate for the PPO training. In this case, the model is `\"lvwerra/gpt2-imdb\"` and the learning rate is set to `1.41e-5`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f181307a-5ccd-40b4-a1b0-666a841eda13"
      },
      "outputs": [],
      "source": [
        "config = PPOConfig(\n",
        "    model_name=\"lvwerra/gpt2-imdb\",\n",
        "    learning_rate=1.41e-5)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed365c09-6716-4ef0-8b40-eeeed73b877c"
      },
      "source": [
        "`config.model_name` refers to the specific model identifier used in the configuration for loading the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dce1ac8f-69b4-4193-a5f1-2bd71da18284"
      },
      "outputs": [],
      "source": [
        "config.model_name"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15bc1453-e3ff-47d8-bd52-b02da3ddc7cf"
      },
      "source": [
        "The `sent_kwargs` dictionary contains parameters for the sentiment analysis pipeline, specifying that all scores should be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703bed10-2b64-404d-bf00-1f9f17ace399"
      },
      "outputs": [],
      "source": [
        "sent_kwargs = {\n",
        "    \"top_k\": None,                    # No limit on top-k results (could mean return all or default behavior)\n",
        "    \"function_to_apply\": \"none\",     # Do not apply any activation function like softmax or sigmoid\n",
        "    \"batch_size\": 2                  # Process inputs in batches of size 2\n",
        "}\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6905c94e-4bcb-4cb2-a860-2e6c9d2f8e69"
      },
      "source": [
        "\n",
        "## AutoModelForCausalLMWithValueHead Overview\n",
        "The `AutoModelForCausalLMWithValueHead` class is used to load the pretrained GPT-2 model with a value head for PPO training.\n",
        "\n",
        "This model **simultaneously** performs two tasks using a shared transformer backbone:\n",
        "\n",
        "### 1. Next Token Prediction\n",
        "- **Function**: Predicts the next token in a sequence, like a standard causal language model (e.g., GPT-2).\n",
        "- **Process**:\n",
        "  - Input tokens are processed through the transformer’s layers.\n",
        "  - Outputs logits (probabilities) over the vocabulary for the next token.\n",
        "- **Component**: Language model head.\n",
        "\n",
        "### 2. Value Score Prediction\n",
        "- **Function**: Produces a scalar value score representing the estimated value or expected reward for the current sequence.\n",
        "- **Process**:\n",
        "  - Uses the transformer’s hidden states (encoded representations of all input tokens).\n",
        "  - Passes hidden states through an additional **value head** (a linear layer) to output a single scalar.\n",
        "- **Component**: Value head.\n",
        "- **Use Case**: Critical for reinforcement learning tasks like RLHF (Reinforcement Learning from Human Feedback) or PPO (Proximal Policy Optimization).\n",
        "\n",
        "## How It Works\n",
        "- **Shared Computation**: Both tasks leverage the same transformer layers, processing input tokens in a single forward pass.\n",
        "- **Dual Outputs**:\n",
        "  - Language model head generates next-token logits.\n",
        "  - Value head produces a value score for the sequence.\n",
        "- **Efficiency**: The model efficiently handles text generation and value estimation, making it ideal for RLHF fine-tuning.\n",
        "\n",
        "---\n",
        "The `AutoTokenizer` class loads the tokenizer that matches a pretrained model.  \n",
        "In this case, the tokenizer's padding token is set to the end-of-sequence (EOS) token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a9718f8-6b50-4012-b441-4fe6e41e8747"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "During PPO training, the model is updated. Additionally, a reference model is employed to stabilize training by incorporating the Kullback-Leibler (KL) divergence between the current policy and the reference policy. The KL divergence serves as a regularization term to prevent excessive deviation from the reference policy."
      ],
      "metadata": {
        "id": "2mSY1AL-HT09"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d8690a6-3fa8-47e8-87cd-884795dd5bb7"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fa95d4-aa71-4d5d-93da-b3c9a4602205"
      },
      "source": [
        "#3. Dataset and processing of Dataset\n",
        "The **IMDB dataset**, containing 50,000 positive or negative movie reviews, is loaded using the datasets library's `load_dataset` function with the \"train\" split for sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad47f05d-563c-4c76-9211-ff5c5fbc9865"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"imdb\"\n",
        "ds = load_dataset(dataset_name, split=\"train\")\n",
        "N = 3\n",
        "for i in range(N):\n",
        "    print(f\"Text: {ds[i]['text']}\")\n",
        "    print(f\"Label: {ds[i]['label']}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9586b93e-3982-4f15-828f-8bec35f446b7"
      },
      "source": [
        "The LengthSampler in the trl (Transformers Reinforcement Learning) library is a utility that helps you randomly vary the length of generated sequences during training.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "402535c2-96b0-4ab4-b49f-0c54f8671c3d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define minimum and maximum input text lengths\n",
        "min_text_length = 2\n",
        "max_text_length = 8\n",
        "\n",
        "# Create a LengthSampler to randomly choose input sizes in the given range\n",
        "input_size = LengthSampler(min_text_length, max_text_length)\n",
        "\n",
        "# Example usage\n",
        "print(input_size())  # Outputs a random number between 2 and 8\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5605bc5-f71d-421f-9e0f-a333138140e8"
      },
      "source": [
        "The function `prepare_text_dataset` loads a text dataset, tokenizes each sample with a randomly chosen input length, and adds a decoded `\"query\"` field. It formats the result for PyTorch, making it ready for model training or fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aacd4c38-4e4b-40f2-8c12-6e61677c28de"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_text_dataset(cfg, dataset_name=\"imdb\", min_text_len=2, max_text_len=8, tokenizer=None):\n",
        "    # Load tokenizer from the model specified in config\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "    # Set padding token to the end-of-sequence token (needed for consistent padding)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load the specified dataset split (default: IMDb training data)\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    # Create a LengthSampler to sample random input lengths between min and max\n",
        "    length_sampler = LengthSampler(min_text_len, max_text_len)\n",
        "\n",
        "    # Define a function to tokenize each sample\n",
        "    def tokenize_function(example):\n",
        "        # Tokenize and truncate the input text to a random length\n",
        "        example[\"input_ids\"] = tokenizer.encode(example[\"text\"])[: length_sampler()]\n",
        "\n",
        "        # Decode the tokens back to text to store as \"query\"\n",
        "        example[\"query\"] = tokenizer.decode(example[\"input_ids\"])\n",
        "        return example\n",
        "\n",
        "    # Apply the tokenize_function to each example in the dataset\n",
        "    dataset = dataset.map(tokenize_function, batched=False)\n",
        "\n",
        "    # Set dataset format to PyTorch tensors\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    return dataset\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93db8c62-8ce4-447b-9d4f-0db42ba84dd4"
      },
      "source": [
        "Create the dataset object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183f1422-3f21-4b53-b2b8-d5744a94169a"
      },
      "outputs": [],
      "source": [
        "dataset = prepare_text_dataset(config)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b955be9b-aa84-43f2-a309-f2a028d92870"
      },
      "source": [
        "print first 5 data points from the dataset, this will give you an idea about the input data needed for training the PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa5388e7-5779-41e8-8116-2d1c5d8850ba"
      },
      "outputs": [],
      "source": [
        "for i, sample in enumerate(dataset):\n",
        "  if i >= 5:\n",
        "    break\n",
        "  print(f\"Sample {i+1}:\")\n",
        "  print(f\"Review: {sample['text']}\")\n",
        "  print(f\"Input IDs: {sample['input_ids']}\")\n",
        "  print(f\"Query: {sample['query']}\")\n",
        "  print(\"-\" * 50)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ec548e-b141-4cb6-a236-6b47d3a5a90e"
      },
      "source": [
        "#4. Collator function\n",
        "🔄 The collator function organizes data into batches for the PPOTrainer by grouping corresponding features from each sample together. ✅ It ensures the input is in the correct format for training.\n",
        "```\n",
        "two samples were input and check the output from collator for the keys  'input_ids', 'query', and 'review'.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57e8f103-8264-4c12-a09f-a9e4a105579d"
      },
      "outputs": [],
      "source": [
        "def collator(samples):\n",
        "    return {field: [sample[field] for sample in samples] for field in samples[0]}\n",
        "\n",
        "\n",
        "examples = [\n",
        "    {'input_ids': [10, 20, 30], 'query': \"hello world\", 'text': \"This product works great!\"},\n",
        "    {'input_ids': [40, 50, 60], 'query': \"test phrase\", 'text': \"I had a fantastic experience.\"}\n",
        "]\n",
        "\n",
        "batched_output = collator(examples)\n",
        "batched_output\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ec88cb-5ca5-4ab0-981e-433d9abb9db9"
      },
      "source": [
        "\n",
        "# 5. Initialize PPOTrainer\n",
        "\n",
        "**Proximal Policy Optimization (PPO)** is a reinforcement learning algorithm well-suited for fine-tuning generative models like chatbots. It addresses common training challenges such as producing stable, coherent, and contextually appropriate responses.\n",
        "\n",
        "#### 🧠 Why PPO for Chatbots?\n",
        "- PPO enhances traditional policy gradient methods by using a **clipped objective function**, which leads to **stable and gradual policy updates**.\n",
        "- This reduces high variance and instability, helping to avoid erratic or inconsistent chatbot behavior.\n",
        "- It maintains a balance between **exploring new responses** and **exploiting known good ones**, thanks to its trust region strategy.\n",
        "\n",
        "#### 🚀 Role of the PPO Trainer\n",
        "- **Collects** dialogue samples during interaction.\n",
        "- **Optimizes** the policy (chatbot behavior) based on those samples.\n",
        "- **Manages** the underlying neural networks during training.\n",
        "\n",
        "This results in a more **robust and reliable** chatbot that responds in a helpful, safe, and aligned manner.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Let’s initialize the `PPOTrainer` with the specified configuration and components:\n",
        "\n",
        "- ```config``` : Configuration settings for PPO training, such as learning rate and model name  \n",
        "- ```model``` : The primary model to be fine-tuned using PPO  \n",
        "- ```tokenizer``` : Tokenizer corresponding to the model, used for processing input text  \n",
        "- ```dataset``` : Dataset to be used for training, providing the input data for the model  \n",
        "- ```data_collator``` : Data collator to handle batching and formatting of the input data\n",
        "````\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9c1d3ac-5811-4f6b-a22b-5fd2669a8950"
      },
      "outputs": [],
      "source": [
        "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b4c01d0-ba37-4598-bc81-5dff1b1f6561"
      },
      "source": [
        "#6. The Reward Function\n",
        "\n",
        "In **Reinforcement Learning with PPO (Proximal Policy Optimization)**, a **reward function** plays a crucial role by providing feedback on the quality of actions taken by the model. For a **generative chatbot**, this means evaluating how good or appropriate its responses are.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Using Sentiment Analysis as a Reward Signal\n",
        "\n",
        "One simple yet effective approach is to use a **sentiment analysis pipeline** as the reward function:\n",
        "\n",
        "- The chatbot's generated response is analyzed using a sentiment classifier.\n",
        "- A **reward score** is assigned based on the **positivity or negativity** of the sentiment.\n",
        "- The PPO Trainer uses this reward to fine-tune the model, encouraging it to generate **more positive and engaging** responses over time.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7117e5d-cda1-4c54-a6e4-5bd12c455be5"
      },
      "source": [
        "### 🛠️ Initialize Sentiment Analysis Pipeline\n",
        "\n",
        "First, we set up a **sentiment analysis pipeline** using a **pretrained model** fine-tuned on IMDB reviews. This model is capable of analyzing input text and predicting sentiment with confidence scores for **positive** and **negative** classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73b46d01-fe93-442f-a12f-45d2e382f940"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis model\n",
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"lvwerra/distilbert-imdb\",\n",
        ")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30502fc0-dc58-42d4-b47e-6351f445af44"
      },
      "source": [
        "The `score` indicates the model’s confidence in its sentiment prediction. A higher score for the **\"POSITIVE\"** class yields a higher reward, while a lower score (or **\"NEGATIVE\"** sentiment) results in a lower or negative reward—encouraging the chatbot to generate more positively perceived responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eb1f2dc-ca4a-4345-bf62-d336f32ca014"
      },
      "outputs": [],
      "source": [
        "text = \"this movie was really bad!!\"\n",
        "sentiment_pipe(text, **sent_kwargs)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62c1cbe7-cc03-42c5-b1b4-7ed0d9865686"
      },
      "source": [
        "#7. Generating responses using PPO\n",
        "\n",
        "##Tokenizing and preparing the input batch\n",
        "This section illustrates the process of generating responses using the PPO (Proximal Policy Optimization) Trainer. It includes tokenizing the input, preparing the training batch, generating model responses, and decoding the output tokens into human-readable text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab8c79e7-1932-4e40-b811-a796eae84fad"
      },
      "source": [
        "The below code  retrieves a\n",
        "- batch of data from the PPO Trainer's dataloader\n",
        "- keys of the data\n",
        "- selects the first two entries for processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23fba0ab-64b8-41e8-8e25-6f89cf827364"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(ppo_trainer.dataloader))\n",
        "#The batch contains label, input_ids, and query\n",
        "print(batch.keys())\n",
        "# Let's take the first two  sample in the batch\n",
        "batch = {key: batch[key][0:2] for key in batch}\n",
        "batch"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59107ac5-77f5-4256-8408-80259b2afeee"
      },
      "source": [
        "Initialize a list named `response_tensors` to store the model-generated responses for scoring. The code below extracts the input_ids from the batch and assigns them to `query_tensors`. These tensors represent the tokenized input sequences, referred to as \"query tensors\" because they serve as the initial queries that the model will process to generate responses in the following steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d2c4999-28cb-4ef6-95a2-5de932b627dc"
      },
      "outputs": [],
      "source": [
        "response_tensors = []\n",
        "query_tensors =  batch[\"input_ids\"]\n",
        "query_tensors"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32b759cf-3aa4-4c69-acdb-a9df8d241cee"
      },
      "source": [
        "The code below defines a lambda function `get_text` that takes a list of response tensors (`response`) and decodes each tensor into readable text using the tokenizer. The `squeeze()` method is applied to remove any singleton dimensions from the tensor before decoding. This allows you to view the original input queries in their human-readable form.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66dbb71b-9e5c-41b7-8518-dc127ddb6f38"
      },
      "outputs": [],
      "source": [
        "get_text = lambda response:''.join([tokenizer.decode(r.squeeze()) for r in response])\n",
        "get_text(query_tensors)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac92cd01-bfe8-4f04-94ab-a0f9b1a30229"
      },
      "source": [
        "The dictionary `generation_kwargs` defines the parameters used for generating sequences from the Language Model (LLM). These parameters control the behavior and diversity of the generated output:\n",
        "\n",
        "- `\"min_length\": -1` – No minimum length is enforced for the generated text.\n",
        "- `\"top_k\": 0.0` – Disables top-k filtering, allowing sampling from all tokens.\n",
        "- `\"top_p\": 1.0` – Disables nucleus (top-p) sampling, using the full probability distribution.\n",
        "- `\"do_sample\": True` – Enables sampling to allow diverse and varied outputs.\n",
        "- `\"pad_token_id\": 50256` – Specifies the padding token ID to ensure uniform sequence length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "692ab3d5-11dc-4465-8d2a-2b0ec78693ca"
      },
      "outputs": [],
      "source": [
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": 50256,\n",
        "}\n",
        "generation_kwargs"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e0fa66-d40e-4ec5-99f4-64ab553cc9ab"
      },
      "source": [
        "The `output_length_sampler` is initialized using `LengthSampler(output_min_length, output_max_length)`. This object samples output lengths for the generated sequences, ensuring they fall within the specified minimum and maximum range. By introducing variability in length, it helps produce more diverse and natural responses, avoiding outputs that are too short or unnecessarily long, thereby improving the overall quality of the model's responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1d49e55-dd30-4e8e-8a18-3c36d7a4174c"
      },
      "outputs": [],
      "source": [
        "output_min_length = 4\n",
        "output_max_length = 16\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "gen_len = output_length_sampler()\n",
        "generation_kwargs[\"max_new_tokens\"] = gen_len"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9473dd6e-6443-417a-bc75-faff51042adf"
      },
      "source": [
        "Now, let's process a single sample using PPO. Begin by extracting the first query tensor from the input batch. Then, generate a response for this query using the PPO trainer along with the specified generation parameters (`generation_kwargs`). The generated response tensor is stored in the variable `response`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a58ef94d-6e17-4274-8f00-021881719ca7"
      },
      "outputs": [],
      "source": [
        "query=query_tensors[0]\n",
        "response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "print(\"query:\",get_text(query))\n",
        "print(\"response:\", get_text(response))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0d3ed30-91da-4d40-a4c5-dcfa6d44118f"
      },
      "source": [
        "Finally, append the generated tokens to the `response_tensors` list. The `squeeze()` method removes any single-dimensional entries from the shape of the tensor, and the slicing `[-gen_len:]` ensures that only the newly generated tokens are included, excluding any tokens that were part of the original input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "865b8965-daef-460a-932b-27a465367e48"
      },
      "outputs": [],
      "source": [
        "response_tensors.append(response.squeeze()[-gen_len:])\n",
        "print(\"newly generated tokens form response:\", get_text(response_tensors[-gen_len:]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a55c913-2b1a-47ca-84bd-201ec4879bfe"
      },
      "source": [
        "Repeat the process for the second sample. This section generates a response for a given query, decodes the relevant part, and appends it to the `response_tensors` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62e8075b-0bf1-4414-b296-332abd65790e"
      },
      "outputs": [],
      "source": [
        "query=query_tensors[1]\n",
        "gen_len = output_length_sampler()\n",
        "generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "tokenizer.decode(response.squeeze()[-gen_len:], skip_special_tokens=True)\n",
        "print(\"query:\",get_text(query))\n",
        "print(\"response ouput :\", get_text(response_tensors))\n",
        "response_tensors.append(response.squeeze()[-gen_len:])\n",
        "print(\"newly generated tokens form response:\", get_text(response_tensors[-gen_len:]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade6ce52-5b97-4b7a-bf20-33e315e31421"
      },
      "source": [
        "Convert each tensor in `response_tensors` into human-readable text and store it in the `batch` dictionary under the key `response`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae2a0e86-16c4-4bbc-bdea-e2b441aaa990"
      },
      "outputs": [],
      "source": [
        "batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "batch[\"response\"]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53eebf8f-301e-436a-adb7-47534be7c6b8"
      },
      "source": [
        "The batch now contains both `response` and `query` keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9301c767-1981-4c7d-9835-ee092566b175"
      },
      "outputs": [],
      "source": [
        "batch"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deff08d8-91fe-44ee-afef-5ff37c6f40cf"
      },
      "source": [
        "## Scoring function\n",
        "\n",
        "Next, prepare the text data for sentiment analysis, which can serve as a component of the reward function in a PPO setup. Sentiment analysis of interactions helps determine the reward signal by evaluating the tone or emotional content of the generated responses.\n",
        "\n",
        "After that, extract the query and response tensors and add them to the batch for further processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0708b302-7d8f-408a-bfbf-4081afae8a38"
      },
      "outputs": [],
      "source": [
        "texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "texts"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f3d6e4a-5013-4401-9c66-d3efe6f5385f"
      },
      "source": [
        "The sentiment scores (`pipe_outputs`) can be used as feedback to update the policy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7365a819-299f-40b5-8151-1f1546378bb5"
      },
      "outputs": [],
      "source": [
        "pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
        "pipe_outputs"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedf371d-f1c5-4fcc-a349-6640ba0358b9"
      },
      "source": [
        "These scores are used to evaluate the quality or relevance of the generated responses, reflecting the model's confidence in their likelihood of being positive. The scores are extracted from the `pipe_outputs` list, where each element contains a list of scores corresponding to the model's output.\n",
        "\n",
        "The code iterates over the `pipe_outputs` list, extracts the score from each output, converts it into a tensor, and stores it in the `rewards` list. These scores serve as a reward signal, representing the model's assessment of how likely the generated responses are to be positive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4375f567-f844-496c-b59c-928ac2ca934c"
      },
      "outputs": [],
      "source": [
        "positive_scores = [\n",
        "    item[\"score\"]\n",
        "    for output in pipe_outputs\n",
        "    for item in output\n",
        "    if item[\"label\"] == \"POSITIVE\"\n",
        "]\n",
        "rewards = [torch.tensor(score) for score in positive_scores]\n",
        "rewards"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111e3a10-aab9-4c5a-8896-708a17762f91"
      },
      "source": [
        "### Proximal policy optimization\n",
        "The training loop executes a single update step of the PPO algorithm. It takes the following inputs:\n",
        "\n",
        "- Query tensor\n",
        "- Response tensor\n",
        "- Score tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2b540d-a9cc-4393-8581-0e762c06f0d0"
      },
      "outputs": [],
      "source": [
        "print(\"query:\", get_text(query_tensors))\n",
        "print(\"\\n\")\n",
        "print(\"response:\", get_text(response_tensors))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82bb36aa-cbfe-4961-9e5c-91baba98a5c0"
      },
      "source": [
        "Here we will create query tensors, response_tensors and rewards of batch size, just to run one step of the PPO trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ac95eb-ed65-42c3-b992-3e9f8050efda"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "query_tensors = pad_list_to_batch_size(query_tensors, batch_size, pad_token_id)\n",
        "\n",
        "response_tensors = pad_list_to_batch_size(response_tensors, batch_size, pad_token_id)\n",
        "rewards=rewards+[torch.tensor(0) for _ in range(batch_size-len(rewards))]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c77d02dc-0eea-4d7b-87ae-1a396b98c9fd"
      },
      "source": [
        "\n",
        "\n",
        "Invoke the PPO step method to update the model using the PPO algorithm with `query_tensors`, `response_tensors`, and `rewards`. It calculates policy and value function losses, computes gradients, and updates policy network parameters to enhance the policy. The method constrains policy updates to prevent large shifts, a key feature of PPO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ce42e6-264c-4fd5-86c8-038f73f4dbaf"
      },
      "outputs": [],
      "source": [
        "stats = ppo_trainer.step(query_tensors, response_tensors, rewards)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1bb3434-5e98-4f91-b294-f808ce4ac917"
      },
      "source": [
        "\n",
        "The `stats` dictionary contains various statistics from the PPO training step, which can be printed using the `print_ppo_stats` function. Its keys are divided into two main categories:\n",
        "\n",
        "- **Minimizing Language Model Loss** (`related_to_objective=True`): Statistics related to optimizing model parameters, such as policy loss and value loss.\n",
        "- **Calculating the Reward**: Metrics relevant to reinforcement learning, including advantage estimates and reward calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "233487e2-0399-4225-b567-2551cd63b2b4"
      },
      "outputs": [],
      "source": [
        "print_ppo_stats(stats, related_to_objective = True)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f04ff481-6098-4155-b2fb-2dca0e2bc618"
      },
      "outputs": [],
      "source": [
        "print_ppo_stats(stats)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e30906c7-0b9b-45e9-9987-619b175e8c03"
      },
      "outputs": [],
      "source": [
        "all_stats = []"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1af02827-0693-4c47-8b51-1b8622bcce1f"
      },
      "source": [
        "The `sentiment`should be set to NEGATIVE for bad responses and POSITIVE for good responses score .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74cd5f09-d16b-4be2-9944-b98341cf9e5c"
      },
      "outputs": [],
      "source": [
        "sentiment = \"POSITIVE\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbdb436-f2a4-4d12-b871-94851eed0599"
      },
      "source": [
        "# Training Loop for PPO with Sentiment Analysis\n",
        "\n",
        "This code implements a training loop for the PPO (Proximal Policy Optimization) algorithm integrated with sentiment analysis. The loop processes batches from the `ppo_trainer` dataloader, executing these steps:\n",
        "\n",
        "1. **Extract Query Tensors**:\n",
        "   - Input IDs (query tensors) are retrieved from the batch.\n",
        "\n",
        "2. **Generate Responses**:\n",
        "   - Responses are generated for each query tensor using `ppo_trainer.generate` with specified `generation_kwargs`.\n",
        "   - Generated responses are decoded and stored in the batch under the `response` key.\n",
        "\n",
        "3. **Compute Sentiment Scores**:\n",
        "   - Queries and responses are concatenated to form text data.\n",
        "   - Sentiment analysis is applied to compute sentiment scores.\n",
        "   - Scores are converted to tensors and stored in the `rewards` list.\n",
        "\n",
        "4. **Run PPO Step**:\n",
        "   - The `ppo_trainer.step` method updates the model using `query_tensors`, `response_tensors`, and `rewards`.\n",
        "   - It computes policy and value function losses, calculates gradients, and updates policy network parameters.\n",
        "   - Policy updates are constrained to prevent large shifts, a core PPO feature.\n",
        "\n",
        "5. **Logging Statistics**:\n",
        "   - Training step statistics are logged and appended to the `all_stats` list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f39ec3d4-32a9-4f82-8b66-3494bc686e9e"
      },
      "outputs": [],
      "source": [
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "    print(f\"epoch {epoch}\")\n",
        "\n",
        "    #### Get response from gpt2\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        gen_len = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "        response_tensors.append(response.squeeze()[-gen_len:])\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    #### Compute sentiment score\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
        "    positive_scores = [\n",
        "           item[\"score\"]\n",
        "           for output in pipe_outputs\n",
        "           for item in output\n",
        "           if item[\"label\"] == sentiment\n",
        "           ]\n",
        "    rewards = [torch.tensor(score) for score in positive_scores]\n",
        "\n",
        "    #### Run PPO step\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "    all_stats.append(stats)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59f2481a-225b-47f9-a1bc-c283edddfa92"
      },
      "outputs": [],
      "source": [
        "# # Save the model\n",
        "\n",
        "model_dir = \"ppo-good\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# # Save model configuration and weights\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e63e52-b4e9-4031-9861-f09f7700c439"
      },
      "source": [
        "#8. Plotting PPO training loss and mean\n",
        "\n",
        "Here's a precise description of the plotting process:\n",
        "\n",
        "1.  **Data Extraction**:\n",
        "\n",
        "    * Isolate the list of total loss values from the `all_stats` dictionary under the key `'total_loss'` and assign it to the variable `loss_values`.\n",
        "    * Isolate the list of mean reward values from the `all_stats` dictionary under the key `'mean_reward'` and assign it to the variable `reward_values`.\n",
        "\n",
        "2.  **Loss Visualization**:\n",
        "\n",
        "    * Generate a line plot where the x-axis represents the training epochs (assuming a corresponding index or epoch list) and the y-axis represents the `loss_values`.\n",
        "\n",
        "3.  **Reward Visualization**:\n",
        "\n",
        "    * Generate a separate line plot where the x-axis represents the training epochs and the y-axis represents the `reward_values`.\n",
        "\n",
        "4.  **Plot Display**:\n",
        "\n",
        "    * Use `plt.tight_layout()` to adjust the spacing between the generated plots for better readability.\n",
        "    * Use `plt.show()` to display the resulting loss and reward plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a1a97b9-63ab-47e4-9ba3-b730b6e8f353"
      },
      "outputs": [],
      "source": [
        "loss_values = [stat['ppo/loss/total'] for stat in all_stats]\n",
        "reward_values = [stat['ppo/mean_scores'] for stat in all_stats]\n",
        "\n",
        "# Plotting the loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(loss_values, label='Total Loss', color='b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('PPO Training Loss over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting the rewards\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(reward_values, label='Mean Reward', color='g')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('PPO Mean Reward over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0927ac17-cb53-419e-9090-54f7a7e65212"
      },
      "source": [
        "#9. Generating and analyzing text with PPO and reference models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "741f34e4-3391-4264-948a-ed0a88eaddcf"
      },
      "source": [
        "**Text generation function**:\n",
        "    - `generate_some_text(input_text, my_model)`: Tokenizes input text, generates a response, and decodes it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e64ec91-25c4-4ca0-b1eb-95e4c68655e0"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available, otherwise use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gen_kwargs = {\"min_length\": -1, \"max_new_tokens\":20, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}\n",
        "def generate_some_text(input_text,my_model):\n",
        "# Tokenize the input text\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "    generated_ids = my_model.generate(input_ids,**gen_kwargs )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text_ = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text_"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c350e8e-4cf8-47bf-8387-4e2348ebb585"
      },
      "source": [
        "**Generate text with PPO model**:\n",
        "    - Generate text using the PPO-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f1b8deb-384b-4785-9a76-2e356939ed94"
      },
      "outputs": [],
      "source": [
        "input_text = \"Once upon a time in a land far\"\n",
        "generated_text=generate_some_text(input_text,model)\n",
        "generated_text"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4a91394-b524-4a86-b51c-20754b93d81a"
      },
      "source": [
        "**Sentiment Analysis**:\n",
        "    - Analyze the sentiment of the generated text using `sentiment_pipe`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8b59be0-59f9-4781-8ca3-2c62202faa52"
      },
      "outputs": [],
      "source": [
        "pipe_outputs = sentiment_pipe(generated_text, **sent_kwargs)\n",
        "pipe_outputs"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff12bc63-740b-4a39-9dec-382aeaa141a4"
      },
      "source": [
        "**Generate text with reference model**:\n",
        "    - Generate text using the reference model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dc162f1-da09-4683-8195-3462e427bb08"
      },
      "outputs": [],
      "source": [
        "generated_text = generate_some_text(input_text,ref_model)\n",
        "generated_text"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4982ce-34b9-496f-8442-34d53476edb0"
      },
      "source": [
        "#10. Comparing PPO and Reference Models\n",
        "\n",
        "This process outlines the comparison of text generation between a PPO-trained model and a reference model:\n",
        "\n",
        "1.  **Generation Parameters**:\n",
        "    * Define `gen_kwargs`: A dictionary containing parameters for text generation (e.g., `max_new_tokens`, `temperature`, `top_k`).\n",
        "\n",
        "2.  **Prepare Batch**:\n",
        "    * Sample a batch of size `bs` from the dataset.\n",
        "    * Extract the query tensors from the sampled batch.\n",
        "\n",
        "3.  **Generate Responses**:\n",
        "    * For each query tensor in the batch:\n",
        "        * Generate a response using the reference model with the defined `gen_kwargs`.\n",
        "        * Generate a response using the PPO model with the defined `gen_kwargs`.\n",
        "\n",
        "4.  **Decode Responses**:\n",
        "    * Convert the generated response tensors from both models into human-readable text strings.\n",
        "\n",
        "5.  **Compute Sentiment Scores**:\n",
        "    * For each query-response pair (before and after PPO training):\n",
        "        * Concatenate the original query and the generated response to form a complete text.\n",
        "        * Use the `sentiment_pipe` to calculate a sentiment score for the generated response.\n",
        "\n",
        "6.  **Store Results**:\n",
        "    * Store the original queries, the generated responses from both models, and their corresponding sentiment scores in a dictionary named `game_data`.\n",
        "    * Convert the `game_data` dictionary into a Pandas DataFrame for easier analysis and return this DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "789370bf-0c12-4171-8535-c5d457a69006"
      },
      "outputs": [],
      "source": [
        "def compare_models_on_dataset(model, ref_model, dataset, tokenizer, sentiment_pipe, sent_kwargs, device, output_length_sampler):\n",
        "    gen_kwargs = {\n",
        "        \"min_length\": -1,\n",
        "        \"top_k\": 0.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"do_sample\": True,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "\n",
        "    bs = 16\n",
        "    game_data = dict()\n",
        "    dataset.set_format(\"pandas\")\n",
        "    df_batch = dataset[:].sample(bs)\n",
        "    game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
        "    query_tensors = df_batch[\"input_ids\"].tolist()\n",
        "\n",
        "    response_tensors_ref, response_tensors = [], []\n",
        "\n",
        "    # Get maximum position embeddings for both models\n",
        "    max_position_embeddings_ref = ref_model.config.max_position_embeddings\n",
        "    max_position_embeddings_model = model.config.max_position_embeddings\n",
        "\n",
        "    for i in range(bs):\n",
        "        gen_len = output_length_sampler()\n",
        "\n",
        "        # Convert query tensors to input IDs\n",
        "        input_ids = torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device)\n",
        "\n",
        "        # ********** Process for ref_model **********\n",
        "        total_length_ref = input_ids.shape[-1] + gen_len\n",
        "        if total_length_ref > max_position_embeddings_ref:\n",
        "            # Truncate input_ids to fit within the max length\n",
        "            max_input_length_ref = max_position_embeddings_ref - gen_len\n",
        "            input_ids_ref = input_ids[:, -max_input_length_ref:]\n",
        "            total_length_ref = input_ids_ref.shape[-1] + gen_len\n",
        "        else:\n",
        "            input_ids_ref = input_ids\n",
        "\n",
        "        output = ref_model.generate(\n",
        "            torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "            max_new_tokens=gen_len,\n",
        "            **gen_kwargs\n",
        "        ).squeeze()[-gen_len:]\n",
        "        response_tensors_ref.append(output)\n",
        "\n",
        "        # ********** Process for model **********\n",
        "        total_length_model = input_ids.shape[-1] + gen_len\n",
        "        if total_length_model > max_position_embeddings_model:\n",
        "            max_input_length_model = max_position_embeddings_model - gen_len\n",
        "            input_ids_model = input_ids[:, -max_input_length_model:]\n",
        "            total_length_model = input_ids_model.shape[-1] + gen_len\n",
        "        else:\n",
        "            input_ids_model = input_ids\n",
        "\n",
        "        output = model.generate(\n",
        "            torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "            max_new_tokens=gen_len,\n",
        "            **gen_kwargs\n",
        "        ).squeeze()[-gen_len:]\n",
        "        response_tensors.append(output)\n",
        "\n",
        "    game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
        "    game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
        "\n",
        "    texts_before = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
        "    game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts_before, **sent_kwargs)]\n",
        "\n",
        "    texts_after = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
        "    game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts_after, **sent_kwargs)]\n",
        "\n",
        "    df_results = pd.DataFrame(game_data)\n",
        "    return df_results"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66311f67-b871-4569-88df-e1be9fd9d947"
      },
      "outputs": [],
      "source": [
        "df_results = compare_models_on_dataset(model, ref_model, dataset, tokenizer, sentiment_pipe, sent_kwargs, device, output_length_sampler)\n",
        "df_results"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bb44f6-c792-48fc-adfd-4eb52f044683"
      },
      "source": [
        "\n",
        "\n",
        "You can also run the PPO  with the sentiment set to NEGATIVE, which evaluates the model's performance when negative sentiment scores are prioritized. The training loop generates responses, computes sentiment scores, updates the model, and logs the statistics for each epoch.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "d28f444c8fb1b6a6530a42effd04d60d2e83c1227d2808395988634aa661157d",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}